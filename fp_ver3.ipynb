{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fp.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator\n",
        "!pip install googletrans"
      ],
      "metadata": {
        "id": "k9jkfi5JJJZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Od1sWB7EOy4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "from googletrans import Translator\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import initializers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import random\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import InputLayer, MaxPooling2D, Flatten, Dense, Conv2D, Dropout, BatchNormalization\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Iy8VplmvG_cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "% cd /content/gdrive/My Drive/Final Project"
      ],
      "metadata": {
        "id": "CzHsXW8RISuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.read_csv('training.csv')\n",
        "test=pd.read_csv('test.csv')\n",
        "valid=pd.read_csv('validation.csv')"
      ],
      "metadata": {
        "id": "TGoXC76AIk_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['text'][0]"
      ],
      "metadata": {
        "id": "EAGEfa_GIy1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx=train.index"
      ],
      "metadata": {
        "id": "HFhNwkzzPweQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.info()"
      ],
      "metadata": {
        "id": "nnhpg-RQXE6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid.info()"
      ],
      "metadata": {
        "id": "ilsmUhoOXJ0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "id": "gNs0ye5oXBQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train2=train.sample(frac=1,random_state=913)\n",
        "train2=train2.reset_index(drop=True)\n",
        "train2"
      ],
      "metadata": {
        "id": "DKOQzNKBYSDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define pipeline translator\n",
        "indo=GoogleTranslator(result='auto',target='id')"
      ],
      "metadata": {
        "id": "F_ytqDj5J7ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx=train2.index"
      ],
      "metadata": {
        "id": "jiGQC_8hhTw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hasil=[]\n",
        "for i in idx:\n",
        "  indos=indo.translate(train2['text'][i])\n",
        "  hasil.append(indos)"
      ],
      "metadata": {
        "id": "E1ZyQRMeQLBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train2['indo']=hasil"
      ],
      "metadata": {
        "id": "dh2toWtqmN3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_indo=train2[['indo','label']]"
      ],
      "metadata": {
        "id": "B3yqJoArmVWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_indo.sample(10)"
      ],
      "metadata": {
        "id": "CE_5Sqdkm-yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "otInaOj0GkDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lowering\n",
        "train_indo['indo']=train_indo['indo'].apply(lambda x: x.lower())\n"
      ],
      "metadata": {
        "id": "jiGevyAHGmNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for i in idx:\n",
        "  indos=indo.translate(train2['text'][i])\n",
        "  hasil.append(indos)"
      ],
      "metadata": {
        "id": "0B_uSX0YN1Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx2=train_indo.index\n",
        "idx2"
      ],
      "metadata": {
        "id": "RRsutwm7OAYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove numbers\n",
        "for i in idx2:\n",
        "  train_indo['indo'][i] = re.sub(r'\\d+', '', train_indo['indo'][i])"
      ],
      "metadata": {
        "id": "muZ34lewJDZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "#storing the puntuation free text\n",
        "train_indo['indo']= train_indo['indo'].apply(lambda x:remove_punctuation(x))"
      ],
      "metadata": {
        "id": "DN0lnCjlOPHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=set(stopwords.words('indonesian'))"
      ],
      "metadata": {
        "id": "Fk8CWK_WQBzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "train_indo['indo_token']=train_indo['indo']\n",
        "for i in idx2:\n",
        "  train_indo['indo_token'][i]=word_tokenize(train_indo['indo'][i])"
      ],
      "metadata": {
        "id": "9F5_UcqQT8FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stop_words]\n",
        "    return output"
      ],
      "metadata": {
        "id": "ZymOwq1-Wxt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_indo['indo_stop']= train_indo['indo_token'].apply(lambda x:remove_stopwords(x))"
      ],
      "metadata": {
        "id": "U48_0QdDVKrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_indo=pd.read_csv('train_indo_token_stopwords.csv')"
      ],
      "metadata": {
        "id": "DO1H574aQHI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_indo.head()"
      ],
      "metadata": {
        "id": "ayHPH-1dQ7Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=train_indo['label']"
      ],
      "metadata": {
        "id": "SMvBuZY42Ucx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, temp_df = train_test_split(train_indo, test_size=0.15,random_state=123,stratify=y)"
      ],
      "metadata": {
        "id": "i4cRPkTiWgmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y2=temp_df['label']"
      ],
      "metadata": {
        "id": "jVxl6uFN3za_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set, valid_set = train_test_split(temp_df, test_size=0.5,random_state=123,stratify=y2)"
      ],
      "metadata": {
        "id": "zemrMkCH3vpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = train_set.indo_stop, train_set.label\n",
        "X_test, y_test = test_set.indo_stop, test_set.label\n",
        "X_valid, y_valid = valid_set.indo_stop, valid_set.label\n",
        "\n",
        "def prepare_target(y_train, y_test,y_valid):\n",
        "  y_train_enc = to_categorical(y_train)\n",
        "  y_test_enc = to_categorical(y_test)\n",
        "  y_valid_enc = to_categorical(y_valid)\n",
        "  return y_train_enc, y_test_enc,y_valid_enc\n",
        "\n",
        "y_train_array, y_test_array,y_valid_array = prepare_target(y_train, y_test,y_valid)\n"
      ],
      "metadata": {
        "id": "kNdX5aobURbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[:10], y_train_array[:10]"
      ],
      "metadata": {
        "id": "KrYyeZ7OY3Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round(sum([len(i.split()) for i in train_set])/len(train_set))"
      ],
      "metadata": {
        "id": "yuPDRtRdb_zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_length = 4000\n",
        "max_length = 20\n",
        "\n",
        "text_vectorization = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                       standardize=\"lower_and_strip_punctuation\",\n",
        "                                       split=\"whitespace\",\n",
        "                                       ngrams=None,\n",
        "                                       output_mode='int',\n",
        "                                       output_sequence_length=max_length)"
      ],
      "metadata": {
        "id": "pDwZ-yTOd4CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vektorisasi teks\n",
        "text_vectorization.adapt(X_train)"
      ],
      "metadata": {
        "id": "YU36V1Qdd-ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sms baru\n",
        "sample = \"aku merasa bahagia, pintar, cerdas, senang\"\n",
        "text_vectorization([sample])"
      ],
      "metadata": {
        "id": "Vrq6AOdxeC8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary()[81]"
      ],
      "metadata": {
        "id": "xIF4mCXNeL7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_text = random.choice(train_set)\n",
        "print(f\"Teks Asli : {random_text}\\n\")\n",
        "text_vectorization([random_text])"
      ],
      "metadata": {
        "id": "d30sfzmkgUMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = layers.Embedding(input_dim=max_vocab_length,\n",
        "                             output_dim=128,\n",
        "                             embeddings_initializer=\"uniform\",\n",
        "                             input_length=max_length)\n",
        "\n",
        "embedding"
      ],
      "metadata": {
        "id": "Itp_jbB5hD-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_text = random.choice(train_set)\n",
        "print(f\"Teks Asli : {random_text}\\n\")\n",
        "text_vectorization([random_text])\n",
        "\n",
        "sample_embedded = embedding(text_vectorization([random_text]))\n",
        "sample_embedded.shape"
      ],
      "metadata": {
        "id": "dF60IziNjgUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorization(inputs)\n",
        "x = embedding(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "hidden_layer1=Dense(500,activation='relu',name='hl1',kernel_initializer=initializers.GlorotNormal(seed=123))(x)\n",
        "batch1=BatchNormalization(axis=1)(hidden_layer1) \n",
        "hidden_layer2=Dense(250,activation='relu',name='hl2',kernel_initializer=initializers.GlorotNormal(seed=123))(batch1)\n",
        "batch2=BatchNormalization(axis=1)(hidden_layer2) \n",
        "hidden_layer3=Dense(125,activation='relu',name='hl3',kernel_initializer=initializers.GlorotNormal(seed=123))(batch2)\n",
        "batch3=BatchNormalization(axis=1)(hidden_layer3) \n",
        "hidden_layer4=Dense(63,activation='relu',name='hl4',kernel_initializer=initializers.GlorotNormal(seed=123))(batch3)\n",
        "batch4=BatchNormalization(axis=1)(hidden_layer4) \n",
        "hidden_layer5=Dense(32,activation='relu',name='hl5',kernel_initializer=initializers.GlorotNormal(seed=123))(batch4)\n",
        "batch5=BatchNormalization(axis=1)(hidden_layer5) \n",
        "outputs = layers.Dense(6, activation='softmax')(batch5)\n",
        "model_1 = tf.keras.Model(inputs, outputs, name='model_nlp_fc')"
      ],
      "metadata": {
        "id": "pEBcpbmckSdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorization(inputs)\n",
        "x = embedding(x)\n",
        "x = layers.LSTM(64)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "hidden_layer1=Dense(500,activation='relu',name='hl1',kernel_initializer=initializers.GlorotNormal(seed=123))(x)\n",
        "batch1=BatchNormalization(axis=1)(hidden_layer1) \n",
        "hidden_layer2=Dense(250,activation='relu',name='hl2',kernel_initializer=initializers.GlorotNormal(seed=123))(batch1)\n",
        "batch2=BatchNormalization(axis=1)(hidden_layer2) \n",
        "hidden_layer3=Dense(125,activation='relu',name='hl3',kernel_initializer=initializers.GlorotNormal(seed=123))(batch2)\n",
        "batch3=BatchNormalization(axis=1)(hidden_layer3) \n",
        "hidden_layer4=Dense(63,activation='relu',name='hl4',kernel_initializer=initializers.GlorotNormal(seed=123))(batch3)\n",
        "batch4=BatchNormalization(axis=1)(hidden_layer4) \n",
        "hidden_layer5=Dense(32,activation='relu',name='hl5',kernel_initializer=initializers.GlorotNormal(seed=123))(batch4)\n",
        "batch5=BatchNormalization(axis=1)(hidden_layer5) \n",
        "outputs = layers.Dense(6, activation='softmax')(batch5)\n",
        "model_1 = tf.keras.Model(inputs, outputs, name='model_nlp_fc')"
      ],
      "metadata": {
        "id": "95PSxFWqAJnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorization(inputs)\n",
        "x = embedding(x)\n",
        "x = layers.GRU(64)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "hidden_layer1=Dense(500,activation='relu',name='hl1',kernel_initializer=initializers.GlorotNormal(seed=123))(x)\n",
        "batch1=BatchNormalization(axis=1)(hidden_layer1) \n",
        "hidden_layer2=Dense(250,activation='relu',name='hl2',kernel_initializer=initializers.GlorotNormal(seed=123))(batch1)\n",
        "batch2=BatchNormalization(axis=1)(hidden_layer2) \n",
        "hidden_layer3=Dense(125,activation='relu',name='hl3',kernel_initializer=initializers.GlorotNormal(seed=123))(batch2)\n",
        "batch3=BatchNormalization(axis=1)(hidden_layer3) \n",
        "hidden_layer4=Dense(63,activation='relu',name='hl4',kernel_initializer=initializers.GlorotNormal(seed=123))(batch3)\n",
        "batch4=BatchNormalization(axis=1)(hidden_layer4) \n",
        "hidden_layer5=Dense(32,activation='relu',name='hl5',kernel_initializer=initializers.GlorotNormal(seed=123))(batch4)\n",
        "batch5=BatchNormalization(axis=1)(hidden_layer5) \n",
        "outputs = layers.Dense(6, activation='softmax')(batch5)\n",
        "model_1 = tf.keras.Model(inputs, outputs, name='model_nlp_fc')"
      ],
      "metadata": {
        "id": "3MGlkf4tsoK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "eXJ-a1c8penW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "id": "UsJ3DmzoppoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_hist = model_1.fit(X_train,\n",
        "                           y_train_array,\n",
        "                           epochs=50,\n",
        "                           validation_data=(X_valid, y_valid_array))"
      ],
      "metadata": {
        "id": "YBj7oO_opyTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model_1, show_shapes=True, rankdir=\"LR\")"
      ],
      "metadata": {
        "id": "AHpsbHV3tOch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results=model_1.predict(X_test)"
      ],
      "metadata": {
        "id": "O1uW6epjrqGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_class=results.argmax(axis=1)"
      ],
      "metadata": {
        "id": "dn7lbhPatq-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, result_class))"
      ],
      "metadata": {
        "id": "1HgJ-TtLzHbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_hist"
      ],
      "metadata": {
        "id": "yWOorjGuqzou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the chart\n",
        "\n",
        "pd.DataFrame(list(zip(model_1_hist.history['accuracy'],model_1_hist.history['val_accuracy'])),\n",
        "               columns =['accuracy', 'val_accuracy']).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dqmAd9z4qaJC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
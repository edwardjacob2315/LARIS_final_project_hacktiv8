model lstm 1
inputs = layers.Input(shape=(1,), dtype="string")
x = text_vectorization(inputs)
x = embedding(x)
x = layers.LSTM(64)(x)
x = Dropout(0.2)(x)
hidden_layer1=Dense(500,activation='relu',name='hl1',kernel_initializer=initializers.GlorotNormal(seed=123))(x)
batch1=BatchNormalization(axis=1)(hidden_layer1) 
hidden_layer2=Dense(400,activation='relu',name='hl2',kernel_initializer=initializers.GlorotNormal(seed=123))(batch1)
batch2=BatchNormalization(axis=1)(hidden_layer2) 
hidden_layer3=Dense(300,activation='relu',name='hl3',kernel_initializer=initializers.GlorotNormal(seed=123))(batch2)
batch3=BatchNormalization(axis=1)(hidden_layer3) 
hidden_layer4=Dense(200,activation='relu',name='hl4',kernel_initializer=initializers.GlorotNormal(seed=123))(batch3)
batch4=BatchNormalization(axis=1)(hidden_layer4) 
hidden_layer5=Dense(100,activation='relu',name='hl5',kernel_initializer=initializers.GlorotNormal(seed=123))(batch4)
batch5=BatchNormalization(axis=1)(hidden_layer5) 
outputs = layers.Dense(6, activation='softmax')(batch5)
model_lstm = tf.keras.Model(inputs, outputs, name='model_nlp_fc')


inputs = layers.Input(shape=(1,), dtype="string")
x = text_vectorization(inputs)
x = embedding(x)
x = layers.LSTM(64)(x)
x = Dropout(0.2)(x)
hidden_layer1=Dense(500,activation='relu',name='hl1',kernel_initializer=initializers.GlorotNormal(seed=123))(x)
batch1=BatchNormalization(axis=1)(hidden_layer1) 
hidden_layer2=Dense(400,activation='relu',name='hl2',kernel_initializer=initializers.GlorotNormal(seed=123))(batch1)
batch2=BatchNormalization(axis=1)(hidden_layer2) 
hidden_layer3=Dense(300,activation='relu',name='hl3',kernel_initializer=initializers.GlorotNormal(seed=123))(batch2)
batch3=BatchNormalization(axis=1)(hidden_layer3) 
hidden_layer4=Dense(200,activation='relu',name='hl4',kernel_initializer=initializers.GlorotNormal(seed=123))(batch3)
batch4=BatchNormalization(axis=1)(hidden_layer4) 
hidden_layer5=Dense(100,activation='relu',name='hl5',kernel_initializer=initializers.GlorotNormal(seed=123))(batch4)
batch5=BatchNormalization(axis=1)(hidden_layer5) 
hidden_layer6=Dense(60,activation='relu',name='hl6',kernel_initializer=initializers.GlorotNormal(seed=123))(batch5)
batch6=BatchNormalization(axis=1)(hidden_layer6) 
hidden_layer7=Dense(30,activation='relu',name='hl7',kernel_initializer=initializers.GlorotNormal(seed=123))(batch6)
batch7=BatchNormalization(axis=1)(hidden_layer7) 
hidden_layer8=Dense(10,activation='relu',name='hl8',kernel_initializer=initializers.GlorotNormal(seed=123))(batch7)
batch8=BatchNormalization(axis=1)(hidden_layer8) 
outputs = layers.Dense(6, activation='softmax')(batch8)
model_lstm = tf.keras.Model(inputs, outputs, name='model_nlp_fc')

inputs = layers.Input(shape=(1,), dtype="string")
x = text_vectorization(inputs)
x = embedding(x)
x = layers.LSTM(64)(x)
x = Dropout(0.2)(x)
hidden_layer1=Dense(100,activation='relu',name='hl1',kernel_initializer=initializers.GlorotNormal(seed=123))(x)
batch1=BatchNormalization(axis=1)(hidden_layer1) 
hidden_layer2=Dense(50,activation='relu',name='hl2',kernel_initializer=initializers.GlorotNormal(seed=123))(batch1)
batch2=BatchNormalization(axis=1)(hidden_layer2) 
hidden_layer3=Dense(25,activation='relu',name='hl3',kernel_initializer=initializers.GlorotNormal(seed=123))(batch2)
batch3=BatchNormalization(axis=1)(hidden_layer3) 
outputs = layers.Dense(6, activation='softmax')(batch3)
model_lstm = tf.keras.Model(inputs, outputs, name='model_nlp_fc')
